+++
date = '2025-06-05T05:30:00+05:30'
draft = false
title = "Tokens: The Language of LLMs"
summary = "LLMs don't directly understand human language. They need text to be converted into tokens - the fundamental units that models actually process and understand."
toc = true
readTime = true
autonumber = true
+++

## What Are Tokens?

When you provide a prompt to an LLM, it doesn't process the raw text directly. Instead, it first breaks the prompt into smaller units called **tokens**. These tokens can be as short as a single character or as long as a commonly used word or phrase. Each token is then converted into a unique numerical ID. This is what the model actually understands and uses to generate responses.

Understanding tokens is crucial because they're the fundamental currency of LLM interactions. Every character you type, every word you use, and every symbol you include gets converted into these numerical representations.

### Token Examples

For example, consider this text snippet. While it contains 252 characters, it breaks down into just 53 tokens:

```
Many words map to one token, but some don't: indivisible.

Unicode characters like emojis may be split into many tokens containing the underlying bytes: ü§öüèæ

Sequences of characters commonly found next to each other may be grouped together: 1234567890
```

You can experiment with tokenization using [OpenAI's tokenizer tool](https://platform.openai.com/tokenizer) to see how your prompt gets broken down.

## How Tokenization Works

Tokenization is the process of converting raw input text into tokens ‚Äî the basic units that LLMs like ChatGPT, Gemini, and Claude understand and process. Before a model can generate a response, your input prompt goes through a tokenizer that maps sequences of characters into numerical IDs.

### Tokenization in Action

For instance, the sentence "I love programming!" might be tokenized as:

```
["I", " love", " programming", "!"]
```

Each token corresponds to a unique number in the model's vocabulary. A massive dictionary containing 10's of thousands to 100's of thousands of tokens.

### Common Tokenization Methods

Different models use various tokenization strategies:

- **Byte Pair Encoding (BPE)**: Combines frequent character pairs to form subwords. Used by GPT models.
- **WordPiece**: Used in BERT, works similarly to BPE by breaking words into subword units.
- **SentencePiece**: Treats input as a raw stream of Unicode characters, often used in multilingual models.

These methods help models efficiently handle rare words, emojis, code, mathematical symbols, and text in multiple languages.

## Why Tokenization Matters
Understanding tokenization helps you interact more effectively with LLMs in several key ways:

* Token Limits: Each model has a context window (e.g., GPT-4 Turbo supports up to 128k tokens). This includes both your prompt and the model‚Äôs reply. Knowing this helps you manage prompt length wisely.

* Cost: Most APIs charge by token usage. Writing concise prompts saves money without sacrificing clarity.

* Quality: Cleanly tokenized prompts avoid truncation and improve output reliability.

### Practical Example
Let's examine how tokenization works with a common sentence:

Input: "The quick brown fox jumps over the lazy dog."

Tokenized version:
```
["The", " quick", " brown", " fox", " jumps", " over", " the", " lazy", " dog", "."]
```
Total: 10 tokens

Notice how spaces are typically included with words (except at the beginning of sentences), and punctuation often becomes separate tokens. This helps the model understand context and proper spacing.

### Thinking in Tokens:
Adopting a token-first mindset helps you write better prompts:

* Be aware of token cost: Rare words, emojis, or long strings (like numbers) can use multiple tokens.

* Be concise: Avoid filler words. Use common, clear language.

* Watch context size: Long inputs reduce space for the model‚Äôs reply. Consider chunking large inputs when needed.

## Key Takeaways
Tokens are how LLMs read and respond to our input. By thinking in tokens and not just words, you can:

* Improve prompt quality

* Stay within context limits

* Cut token processing costs

* Get better results

The more token-aware you are, the more powerful your interaction with LLMs becomes.

